# Deep Learning with Differential Privacy - Experiment Reproduction

This project reproduces the experiments from the paper "Deep Learning with Differential Privacy" by Abadi et al. using TensorFlow Privacy.

## ğŸ¯ Project Overview

This implementation provides:
- **DP-SGD Training**: Using TensorFlow Privacy's `DPKerasSGDOptimizer`
- **CNN Model**: Reproducing the paper's CNN architecture for MNIST
- **Privacy Analysis**: Computing privacy budgets (Îµ, Î´) for different configurations
- **Comprehensive Evaluation**: Multiple privacy settings with accuracy measurements
- **Publication-Quality Visualizations**: Privacy-accuracy trade-off plots

## ğŸ“ Project Structure

```
privacy/
â”œâ”€â”€ config.py              # Experiment configurations and hyperparameters
â”œâ”€â”€ data_loader.py          # MNIST data loading and preprocessing
â”œâ”€â”€ models.py              # CNN model architecture
â”œâ”€â”€ dp_training.py         # Differential privacy training logic
â”œâ”€â”€ run_experiments.py     # Main experiment runner
â”œâ”€â”€ visualization.py       # Results visualization and plotting
â”œâ”€â”€ requirements.txt       # Python dependencies
â””â”€â”€ README.md             # This file
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Run All Experiments

```bash
python run_experiments.py
```

This will run experiments with different privacy budgets:
- Non-private baseline
- Strong privacy (Îµ=2.0)
- Medium privacy (Îµ=4.0) 
- Weak privacy (Îµ=8.0)
- Very weak privacy (Îµ=16.0)

### 3. Generate Visualizations

```bash
python visualization.py
```

### 4. Run Single Experiment

```bash
python run_experiments.py --config strong_privacy
```

## ğŸ“Š Expected Results

The experiments will generate:

1. **Privacy-Accuracy Trade-off Plot**: Shows how test accuracy decreases as privacy increases
2. **Training Curves**: Training/validation accuracy over epochs for each privacy setting
3. **Noise Impact Analysis**: Effect of noise multiplier on model performance
4. **Summary Table**: Comprehensive results table with all metrics

## ğŸ”§ Configuration

### Privacy Settings

Edit `config.py` to modify privacy parameters:

```python
DP_CONFIGS = [
    {
        'name': 'custom_privacy',
        'l2_norm_clip': 1.0,           # Gradient clipping threshold
        'noise_multiplier': 1.2,       # Noise level
        'epsilon': 3.0,                # Target privacy budget
        'delta': 1e-5,                 # Privacy parameter
        'microbatches': 250            # Number of microbatches
    }
]
```

### Model Architecture

The CNN model follows the paper specification:
- Conv1: 16 filters, 8Ã—8 kernel, stride 2, tanh activation
- Conv2: 32 filters, 4Ã—4 kernel, stride 2, tanh activation  
- Dense: 32 units, tanh activation
- Dropout: 0.25
- Output: 10 classes, softmax activation

### Training Parameters

- **Batch Size**: 250 (as in paper)
- **Epochs**: 60
- **Learning Rate**: 0.15
- **Momentum**: 0.9

## ğŸ“ˆ Understanding Results

### Privacy Budget (Îµ, Î´)
- **Lower Îµ**: Stronger privacy, typically lower accuracy
- **Higher Îµ**: Weaker privacy, typically higher accuracy
- **Î´**: Probability of privacy breach (usually 1e-5)

### Key Metrics
- **Test Accuracy**: Final model performance on test set
- **Privacy Spent**: Actual (Îµ, Î´) computed by TF Privacy
- **Training Time**: Time to complete training

## ğŸ”¬ Experiment Details

### Differential Privacy Implementation
- Uses `DPKerasSGDOptimizer` from TensorFlow Privacy
- Implements per-example gradient clipping
- Adds calibrated Gaussian noise to gradients
- Computes privacy spent using RDP analysis

### Privacy Accounting
- Uses RÃ©nyi Differential Privacy (RDP) for tight analysis
- Converts RDP to (Îµ, Î´)-DP for interpretability
- Accounts for composition over training steps

## ğŸ“‹ Results Interpretation

Expected accuracy ranges (approximate):
- **Non-private**: ~99% test accuracy
- **Îµ=16**: ~97-98% test accuracy  
- **Îµ=8**: ~95-97% test accuracy
- **Îµ=4**: ~92-95% test accuracy
- **Îµ=2**: ~85-92% test accuracy

## ğŸ›  Troubleshooting

### Common Issues

1. **Memory Errors**: Reduce batch size in `config.py`
2. **Slow Training**: Reduce number of epochs for testing
3. **Import Errors**: Ensure TensorFlow Privacy is installed correctly

### Performance Tips

- Use GPU for faster training: `pip install tensorflow-gpu`
- Reduce dataset size for quick testing
- Adjust microbatch size based on available memory

## ğŸ“š References

- [Deep Learning with Differential Privacy](https://arxiv.org/abs/1607.00133) - Abadi et al.
- [TensorFlow Privacy](https://github.com/tensorflow/privacy) - Official implementation
- [Differential Privacy](https://en.wikipedia.org/wiki/Differential_privacy) - Background

## ğŸ¤ Contributing

Feel free to:
- Add new model architectures
- Implement additional datasets
- Improve visualization plots
- Add more privacy analysis tools

## ğŸ“„ License

This project is for educational and research purposes, reproducing published academic work.
